{
  
    
        "post0": {
            "title": "Building Infrastructure for Web- and AI-powered Data Analysis",
            "content": "A future of web- and AI-powered data analysis . Wei Ouyang (@oeway) from SciLifeLab | KTH Royal Institute of Technology, Stockholm shared his work on building infrastructure for web- and AI-powered data analysis. This includes: . ImJoy and browser computing | Hypha and BioEngine | BioImage Model Zoo | AI-assisted BioImage Analysis | . Interactive slides with built in demos can be found at this link. . .",
            "url": "https://janelia-cellmap.github.io/cellmap-hackathon-blog/markdown/2022/06/13/imjoy-presentation-weiOUYANG.html",
            "relUrl": "/markdown/2022/06/13/imjoy-presentation-weiOUYANG.html",
            "date": " • Jun 13, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://janelia-cellmap.github.io/cellmap-hackathon-blog/jupyter/2022/06/12/test.html",
            "relUrl": "/jupyter/2022/06/12/test.html",
            "date": " • Jun 12, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Meet the attendees",
            "content": "Meet the hackathon attendees . David Ackerman @davidackerman . I am a software engineer/data analyst interested in teasing out useful information from complex datasets, regardless of domain. I went from computational astrophysics research in undergrad, to experimental and computational membrane biophysics research in grad school, to software engineering at Janelia. As a member of Scientific Computing at Janelia, I have worked on a variety of projects involving large-scale image processing and analysis, including FlyEM and CellMap, where my work has ranged from image alignment to segmentation analysis. I look forward to learning about what everyone else works on, and getting more involved on the machine learning side of things. . Diane Adjavon . Brand new machine learning researcher at Janelia with a focus on computer vision tools for better microscopy annotation. I worked on predicting fluorescence from bright-field images in grad school, where I learned the importance of human interpretation and interaction on performance, both qualitatively and quantitatively. During the hackathon I’m hoping to dive into the possibilities for human-machine interaction for better training, combining fine-tuning and transfer learning, annotation tools, and active learning. . Davis Bennett @d-v-b . I’m a data engineer in the CellMap project team. I work on our data sharing site (OpenOrganelle) and our general data storage strategy. My “wish list” currently includes: . standardize our spatial metadata / project storage layout to something community-driven (e.g. OME-NGFF) | Implement a backend for our ML models such that they present their output as a lazy chunked data source that can be viewed with chunk-friendly visualization tools (neuroglancer, bigdataviewer) | Get the aforementioned backend running on aws | Easy lossy image compression for zarr / n5 arrays (e.g., jpeg) | Make multiresolution pyramids on GPUs | . John Bogovic @bogovic . Computer scientist in the Saalfeld lab whose research focuses primarily on image registration, especially inter-modality (e.g. CLEM). Develops and maintains BigWarp, and contributes to imglib2, bigdataviewer, and n5. Denizen of forum.image.sc. During the hackathon, I plan to continue developing a OME-NGFF’s standard for coordinate transformations, and hope to hear about everyone’s approaches to data wrangling, with an eye to using metadata to make our coding lives happier. . Genevieve Buckley @GenevieveBuckley . Scientist and programmer from CryoEM facility at Monash University in Melbourne, Australia. Maintainer for dask-image and napari. Possible projects (let me know if you want to team up on anything, or have your own project idea you’d like to work on together): . Training and prediction on large CryoEM volumes Priority: Our facility wants to do organelle prediction (especially for FIBSEM datasets), following in the footsteps of Heinrich et al. | . | Annotation tools Paintera (I’m new to painters, but would like to use it better) Questions: Editing labels generated outside paintera, I can’t erase label areas. (I can erase labels drawn in the painters session) | Documentation improvements: I’m offering to make tutorial videos | Development: I’d like to upgrade the version of JavaFX Paintera uses, so it can run on my M1 Macbook. Some help needed setting up my dev env. | . | Zarpaint napari plugin. annotate directly into a zarr file. We are currently extending zarpaint to add slice interpolation features, see https://github.com/GenevieveBuckley/slice-interpolation | . | . | Big data handling dask: to_n5 function (to match to_zarr, to_parquet, etc.) | dask-image: multi-resolution dataset generator (given a full resolution dataset, compute lower resolutions and save to N5/zarr file) | dask-image: switch imread to use imageio instead of pims under the hood | . | . Ryan Conrad @conradry . I’m a software engineer at the NIH focused on deep learning. Recent projects include the CEM500K and CEM1.5M datasets for unsupervised pre-training and transfer learning, the CEM-MitoLab dataset for training instance segmentation models to detect mitochondria, and the empanada library and corresponding napari plugin for panoptic segmentation of 2D and 3D EM data (especially FIBSEM). Lately, I’ve been transitioning my data pipeline to work with OME-Zarr and am very interested in the associated metadata discussion. Really, all of the projects are interesting to me but ML and annotation tools are near the top of the list. Another topic of interest, that wasn’t mentioned, are methods for integrating existing ML datasets with different label spaces/ontologies in order to train more robust and general models. . Draga Doncila Pop @DragaDoncila . Computer science PhD student from Monash University in Melbourne, Australia, working on an interactive cell tracking annotation and segmentation framework, and napari core developer. I want to learn how current tools allow for interactive annotation on large volumes and how we can improve the standardization of data formats and their metadata for storage, transfer and visualization. Our ideal segmentation and tracking solution would be capable of taking natural annotations and corrections on volume sections and using those to predict unseen sections and correct existing ones. We would prefer to keep a relatively simple learning model for feature extraction in favor of a rapid and robust optimization model that can solve on the fly and incorporate human annotations for rapid re-solve. In general: . Image data formats and metadata | Training &amp; prediction on large volumes | Annotation tools | . Jan Funke @funkey . Computer vision and machine learning researcher at Janelia, working on applications to all sorts of microscopy image analysis. In the context of this hackathon, we are interested in building interactive machine learning methods, which we call “life-supervised learning”. . A life-supervised machine learning method learns on the fly as a user annotates a volume. It provides incremental updates to so far not annotated areas to either proof-read or accept as-is. In such a system, there is no difference between manual annotation of structures of interest, ground-truth generation, and proof-reading: As an annotator opens a new dataset and starts segmenting structures of interest, a machine learning algorithm is trained in the background to continuously make predictions in so far unannotated areas. Just as for humans, the training of the model is a continuous, life-long process. Over time, the algorithm gradually improves and the annotator transitions from teaching the model to correcting errors made by the model due to model error or data noise. Importantly, and in contrast to conventional pipelines, the training signal for the model is highly focused throughout the whole lifetime: Only structures that were erroneously predicted need to be corrected, and only those corrections are used to refine the model. . Such a system could be realized by splitting the machine learning method into two parts: (1) a slow, high-capacity learner to predict semantically meaningful embeddings, and (2) a fast, low-capacity learner to turn the embeddings into the target (e.g., a semantic segmentation). This separation will allow the system to be: . Interactive: Corrections made by a user should quickly proliferate to other, not annotated areas. This will be realized by the fast, low-capacity learner. | Consistently Improving: the system should keep learning as more annotations are available. This will be realized by the slow, high-capacity learner, which will keep continuously improving the semantic embeddings on all available annotations. | Persistent: User annotations should never be overwritten, even if other, similar structures have been annotated differently. This will be realized by extrapolation from annotated data points (i.e., transductive learning). | . Kyle Harrington @kephale . Computer scientist working as a senior research scientist at Oak Ridge National Lab. Web: https://kyleharrington.com . I have big zarr images on an HPC that need to be segmented, but no ground truth labels. At the hackathon I am cleaning up my workflow for general usage. In this workflow I use napari to interactively paint and train models + predict segmentations in HPC environments. . Topics of interest: . Image data formats and metadata | Backend development | Scalable ML [networks] | ML finetuning and transfer learning | Training and prediction on large volumes | Annotation tools | . Larissa Heinrich @neptunes5thmoon . I’m a PhD student in the Saalfeld lab working on machine learning problems for volume electron microscopy data. Specifically I have worked on super resolution to improve axial resolution in serial section data and segmentation of synapses and organelles. I’m interested in developing network architectures that can effectively utilize large receptive fields. Further I’m hoping to work on transfer learning, self-supervised pre-training, finetuning or anything else that might help reduce the amount of training data we need. I also thoroughly enjoy discussions about the ongoing efforts regarding metadata standards. . Caleb Hulbert . I am a software engineer in the Saalfeld lab, working primarily on Paintera, and helping maintain lab projects. My background was initially in biology, working in both a maize genetics lab, and a computational neuroscience lab in the past, before focusing more on software development. I’m interested in working on annotation tools for this hackathon, and engaging in active learning. I’m also interested in some of the ongoing efforts regarding image data formats and metadata. . Dagmar Kainmueller @kainmueller . I am a computer scientist working on machine learning for image analysis, mainly instance- and panoptic segmentation. Towards generalizable / interactive ML, we’re looking into combining (or unifying?) components like cycleGAN, semi-supervised pipelines like momentum contrast, Bayesian DL, XAI, also RFs for fast user feedback, etc. Also very interested in existing / upcoming infrastructure (we’ve been playing with napari, paintera, ilastik) wondering where and how to best integrate our models . Dominik Kutra @k-dominiki . I’m a research software engineer in Anna Kreshuk’s lab at EMBL Heidelberg. I work on ilastik, so accessible machine learning is close to my heart. I’m interested in making software easy to use (both for developers and non-coding users) - all aspects from algorithmic performance, UX/API design, deployment. The latter is especially challenging with the neural networks. Other topics I’m interested in: . Interoperability | Interactive deep learning | . Caroline Malin-Mayor . I am a computer vision researcher currently getting my PhD at University of Toronto. Previously I was a Scientific Computing Associate working in the Funke lab on cell tracking from sparsely annotated data in large light-sheet datasets of drosophila, zebrafish, and mouse. I am most interested in making this work more accessible to other researchers and end-users, which is challenging due to the size of the datasets and complexity of the algorithm. More generally, this falls under training/prediction on large volumes and backend development, as well as fine tuning the pre-trained models for application to new datasets. . Tri Nguyen . Hi! I have been a postdoc in the Lee Lab at Harvard Medical School for about 4 years now working on segmentation of a cerebellum. Since I came to Janelia in 2018 to work with Jan on Daisy and then the culmination of an analysis of the cerebellum, I have pretty much touched on every aspects of segmentation and am now excited to work on decreasing the cost of segmentation through overcoming imaging defects/misalignments, transfer learning through cyclegan, superresolution, cheap groundtruthing, proofreading, etc. In one particular topic, I am excited to learn and brainstorm with people on how to tackle the issue of continual learning as it is well known in ML world that today’s architectures are prone to forget when exclusively presented with novel training samples. . Wei Ouyang @oeway . I am currently a researcher at Science for Life Laboratory and KTH Royal Insitute of Technology, Stockholm, Sweden.I developed ImJoy for building interactive tools using web and python. I am coordinating a collaborative effort of BioImage Model Zoo which aims to build a platform for sharing AI models in bioimage analysis. I am responsible for building cloud computing infrastructure to enable test runs of models in the BioImage Model Zoo. I am working on the BioEngine and Hypha for allowing scaling AI models and application serving. . For the hackathon, I am interested in discussing with the CellMap team and others about: . AI model sharing standard and the BioImage Model Zoo | Scalable AI model serving (inference and training) in the cloud | Interactive model training, for collaboration between research groups, public AI challenges or citizen science | ImJoy, web-based image analysis, web assembly, web based image visualization and annotation | . Will Patton @pattonw . I am a software engineer / machine learning practitioner interested in tooling and algorithm development for applying machine learning quickly, easily, and efficiently. I did my undergrad in Mathematics and Computational Modeling, and have worked at Janelia since graduating in May 2018. Since joining Janelia I have worked with a couple labs and project groups, most recently as a part of Scientific Computing and the CellMap project team. Of particular interest to me are the model related categories (finetuning/transfer learning, training/prediction on large volumes, scalable ML networks). I am very excited to collaborate with the other participants, and learn from their work. . Tobias Pietzsch @tpietzsch . I’m a computer scientist working as a freelancer for Scientific Computing at Janelia (among others). I develop and maintain ImgLib2 and BigDataViewer, and contribute to Mastodon, MaMuT, Labkit, etc. I’m interested in all aspects of image processing, analysis, visualization, and storage. (But in practice, I mostly work on tools for interactive visualization and annotation.) For the hackathon, I would like to see what everybody is working on. I would like to learn more about the active learning project, i.e., the big picture and where our tools fit in. I would also love to do some hacking. Random topic selection: . support images with masks (alpha/binary) in ImgLib2/BigDataViewer, | implement HTTP backend for N5 | support HDF5 backend for BDV on Apple ARM64 | finish work on improving BDV UI on HiDPI screens I’m also willing to dive into OME-NGFF and metadata discussions, if I’m forced to… :smile: | . Stephan Preibisch @StephanPreibisch . I am a computer scientist that also worked for some years experimentally with single molecule techniques. The major projects I contributed to so far are ImgLib2, BigStitcher, Render, RS-FISH, STIM and some more. I am the director of Scientific Computing at HHMI Janelia and very interested in reconstruction and analysis of very large (peta-scale) lightsheet and electron microscopy datasets, machine learning, and Open Science Software in general. For the hackathon I would like to get an overview of everyone’s work and would like to contribute to our new active learning project and integrate existing tools such as BigStitcher better with downstream tools for image segmentation. . Franz Rieger @riegerfr . I’m a first year PhD student in Jörgen Kornfeld’s lab working on ML for connectomics. We try to take advantage of the vast amounts of unlabelled data via self supervised pre-training of segmentation models. To give back to the ML community we are aiming to improve the segmentation architectures and we are also working on more biologically plausible self-supervised learning approaches. During the hackathon I would like to explore novel segmentation approaches. . Konrad Rokicki @krokicki . I’m the manager of software engineering in Scientific Computing, and I created the Janelia Workstation for FlyLight and MouseLight, for enabling end-user access to large collections of bioimage data sets. Davis, Aubrey and I have discussed prototyping support for CellMap data sets in Workstation. I’ve begun by adding functionality to make remote n5/zarr containers available for browsing in the Workstation, and Davis is working on creating a database for the data set metadata. The next step is for me to integrate BigDataViewer into the Workstation, to be able to quickly view any data set with the click of a button, without the user having to think about file paths or data locality. During the hackathon, I’d like to benefit from the groups’ collective experience and insight to guide my integration of BDV, and to discuss what other high-level functionality might be useful for further enabling accessibility. . Stephan Saalfeld @axtimwalde . Computer Scientist interested in image analysis (of neurons and other cells, also any other image data) and open source software. Really likes the size, dazzling richness, and confusing artifacts of large 3D electron microscopy. Wrote some parts of ImgLib2, BigDataViewer, Paintera, and a few other open source tools. Currently group leader at Janelia working on scalable image analysis of large 3D EM data together with CellMap project team and others. I would love to learn about everybody else’s work and integrate your ideas and tools with our ideas and tools, e.g. have Paintera interactively generate cellular organelle predictions, edit them and start retraining, or interactively use contour predictors to aid manual annotation. . Deborah Schmidt @frauzufall . Research Software Engineer for Helmholtz Imaging with experience in human computer interaction, data visualization, ImageJ2 / SciJava core development, usability, DevOps, reproducibility. Together with Martin I was part of a team publishing a whole beta cell reconstruction, including Blender visualizations. I work with Kyle on Album for sharing reproducible software solutions via single python files. . Looking forward to meeting people and potentially working on the Label Editor (label visualization based on BDV), integrating the existing beta cell segmentation strategies into Album solutions, compatibility of Album solutions with other frameworks, alo happy to do visualizations and open to other ideas. . Arlo Sheridan @sheridana . Machine learning researcher working on various segmentation, restoration and multi-object tracking projects at the Salk Institute. Previously worked in the Funke lab at Janelia on neuron segmentation in large EM volumes. I am interested in generalizable/scalable networks, transfer + active learning, and visualization. . Aubrey Weigel @avweigel . I started grad school as a physicist building microscopes to study diffusion of protein channels on the plasma membrane. I then spent my postdoc looking inside the cell at the ER, Golgi, and everything in between. Over time I’ve shifted from using light microscopy to study dynamics in living systems to using electron microscopy to study morphologies and ultrastructure in ‘not-so-living’ systems. I now lead the CellMap project team, an evolution of the COSEM project team, to interrogate the 3D architecture and subcellular structures of cells in tissues using approaches like volume EM, CLEM, and expansion microscopy. Maybe not surprisingly, I’m interested - as both a user and project manager - in all areas of the proposed topics. . Martin Weigert @maweigert . I am a computer scientist with an interest in machine learning based image reconstruction and segmentation, as well as the development of open source python tools such as CSBDeep and stardist. I am currently a group leader at EPFL, where my team is focused on developing unsupervised methods for ml-based bioimage analysis and the extension of segmentation methods to different data modalities. For the hackathon, I am (above all) interested in exchange of scientific ideas and discussing fundamental limitations of current approaches (both software and methods wise). . Marwan Zouinkhi @mzouink . I started as a software engineer and a mobile developer then I shifted into scaling image analysis for my PHD. Last November, I joined Janelia as part of the Scientific computing team and CellMap project. I am interested in scaling image processing of large multi-dimensional datasets and automatizing image reconstructions. Currently, I am working on a versioned data store service that will be used in the active learning project. During the hackathon I plan to integrate the versioned storage into Paintera and ML applications. .",
            "url": "https://janelia-cellmap.github.io/cellmap-hackathon-blog/markdown/2022/06/06/introductions.html",
            "relUrl": "/markdown/2022/06/06/introductions.html",
            "date": " • Jun 6, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "CellMap Hackathon 2022 . The aim of this hackathon is to enable active developers of established and emerging tools for interactive annotation and machine learning (ML) on large volumetric data to: . share their experiences and help each other understand their code, | discuss open problems and evaluate existing solutions, | help each other improving their tools, | identify and/ or establish interfaces for interaction between tools and services, | identify best practices and leaders on various common issues | . Specifically, we intend to focus on topics such as image data formats and metadata, versioning of data, backend development, (scalable) ML networks, ML finetuning and transfer learning, training and prediction on large volumes, annotation tools, and active learning. . As these are emerging fields, it is not expected that we will identify the best possible solution for each category, however, honest discussions with the goal to identify the most promising direction are expected. We intend to learn about existing approaches and help each other use or adapt well working solutions. . The hackathon will not have a scheduled agenda other than meals and a break on the weekend. Ad-hoc presentations of some participants are expected as need arises but will not be scheduled ahead of time. We are not planning to present results at the end of the hackathon as it competes with working on actual goals. The organizers will assemble a summarizing report after the hackathon and will collaborate remotely with participants in the process. This report will be published in a freely accessible form (blog or preprint). . It is not necessary to prepare an agenda for the hackathon as we expect it to develop organically. However, we will start communicating before the hackathon begins, and all attendees are more than welcome to start shaping the agenda with pressing issues from their perspective. .",
          "url": "https://janelia-cellmap.github.io/cellmap-hackathon-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://janelia-cellmap.github.io/cellmap-hackathon-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}