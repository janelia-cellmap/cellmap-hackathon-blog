<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Meet the attendees | CellMap hackathon blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Meet the attendees" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A breif intoduction to all the hackathon attendees." />
<meta property="og:description" content="A breif intoduction to all the hackathon attendees." />
<link rel="canonical" href="https://janelia-cellmap.github.io/cellmap-hackathon-blog/introductions/about/2022/06/06/introductions.html" />
<meta property="og:url" content="https://janelia-cellmap.github.io/cellmap-hackathon-blog/introductions/about/2022/06/06/introductions.html" />
<meta property="og:site_name" content="CellMap hackathon blog" />
<meta property="og:image" content="https://janelia-cellmap.github.io/cellmap-hackathon-blog/images/group-dinner-2.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-06-06T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://janelia-cellmap.github.io/cellmap-hackathon-blog/images/group-dinner-2.jpg" />
<meta property="twitter:title" content="Meet the attendees" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-06-06T00:00:00-05:00","datePublished":"2022-06-06T00:00:00-05:00","description":"A breif intoduction to all the hackathon attendees.","headline":"Meet the attendees","image":"https://janelia-cellmap.github.io/cellmap-hackathon-blog/images/group-dinner-2.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://janelia-cellmap.github.io/cellmap-hackathon-blog/introductions/about/2022/06/06/introductions.html"},"url":"https://janelia-cellmap.github.io/cellmap-hackathon-blog/introductions/about/2022/06/06/introductions.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/cellmap-hackathon-blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://janelia-cellmap.github.io/cellmap-hackathon-blog/feed.xml" title="CellMap hackathon blog" /><link rel="shortcut icon" type="image/x-icon" href="/cellmap-hackathon-blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper">
<a class="site-title" rel="author" href="/cellmap-hackathon-blog/"><img src="/cellmap-hackathon-blog/images/logo.png" alt="CellMap hackathon blog logo"> <b>CellMap hackathon blog</b></a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger">
<a class="page-link" href="/cellmap-hackathon-blog/about/">About</a><a class="page-link" href="/cellmap-hackathon-blog/search/">Search</a><a class="page-link" href="/cellmap-hackathon-blog/categories/">Tags</a>
</div>
      </nav>
</div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Meet the attendees</h1>
<p class="page-description">A breif intoduction to all the hackathon attendees.</p>
<p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-06-06T00:00:00-05:00" itemprop="datePublished">
        Jun 6, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      17 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i> 
      
        <a class="category-tags-link" href="/cellmap-hackathon-blog/categories/#introductions">introductions</a>
         
      
        <a class="category-tags-link" href="/cellmap-hackathon-blog/categories/#about">about</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="meet-the-hackathon-attendees">Meet the hackathon attendees</h1>

<h2 id="david-ackerman-davidackerman">David Ackerman <a href="https://github.com/davidackerman">@davidackerman</a>
</h2>
<p>I am a software engineer/data analyst interested in teasing out useful information from complex datasets, regardless of domain. I went from computational astrophysics research in undergrad, to experimental and computational membrane biophysics research in grad school, to software engineering at Janelia. As a member of Scientific Computing at Janelia, I have worked on a variety of projects involving large-scale image processing and analysis, including FlyEM and CellMap, where my work has ranged from image alignment to segmentation analysis. I look forward to learning about what everyone else works on, and getting more involved on the machine learning side of things.</p>

<h2 id="diane-adjavon">Diane Adjavon</h2>
<p>Brand new machine learning researcher at Janelia with a focus on computer vision tools for better microscopy annotation. I worked on predicting fluorescence from bright-field images in grad school, where I learned the importance of human interpretation and interaction on performance, both qualitatively and quantitatively.
During the hackathon I’m hoping to dive into the possibilities for human-machine interaction for better training, combining fine-tuning and transfer learning, annotation tools, and active learning.</p>

<h2 id="davis-bennett-d-v-b">Davis Bennett <a href="https://github.com/d-v-b">@d-v-b</a>
</h2>
<p>I’m a data engineer in the CellMap project team. I work on our data sharing site (<a href="https://www.lee.hms.harvard.edu/">OpenOrganelle</a>) and our general data storage strategy. My “wish list” currently includes:</p>
<ul>
  <li>standardize our spatial metadata / project storage layout to something community-driven (e.g. OME-NGFF)</li>
  <li>Implement a backend for our ML models such that they present their output as a lazy chunked data source that can be viewed with chunk-friendly visualization tools (neuroglancer, bigdataviewer)</li>
  <li>Get the aforementioned backend running on aws</li>
  <li>Easy lossy image compression for zarr / n5 arrays (e.g., jpeg)</li>
  <li>Make multiresolution pyramids on GPUs</li>
</ul>

<h2 id="john-bogovic-bogovic">John Bogovic <a href="https://github.com/bogovicj">@bogovic</a>
</h2>
<p>Computer scientist in the Saalfeld lab whose research focuses primarily on image registration, especially inter-modality (e.g. CLEM). Develops and maintains BigWarp, and contributes to imglib2, bigdataviewer, and n5. Denizen of forum.image.sc. During the hackathon, I plan to continue developing a OME-NGFF’s standard for coordinate transformations, and hope to hear about everyone’s approaches to data wrangling, with an eye to using metadata to make our coding lives happier.</p>

<h2 id="genevieve-buckley-genevievebuckley">Genevieve Buckley <a href="https://github.com/GenevieveBuckley">@GenevieveBuckley</a>
</h2>
<p>Scientist and programmer from CryoEM facility at Monash University in Melbourne, Australia. Maintainer for <a href="https://www.lee.hms.harvard.edu/">dask-image</a> and <a href="https://www.lee.hms.harvard.edu/">napari</a>. 
Possible projects (let me know if you want to team up on anything, or have your own project idea you’d like to work on together):</p>
<ul>
  <li>Training and prediction on large CryoEM volumes
    <ul>
      <li>Priority: Our facility wants to do organelle prediction (especially for FIBSEM datasets), following in the footsteps of <a href="https://www.nature.com/articles/s41586-021-03977-3">Heinrich et al.</a>
</li>
    </ul>
  </li>
  <li>Annotation tools
    <ul>
      <li>Paintera (I’m new to painters, but would like to use it better)
        <ul>
          <li>Questions: Editing labels generated outside paintera, I can’t erase label areas. (I <em>can</em> erase labels drawn in the painters session)</li>
          <li>Documentation improvements: I’m offering to make tutorial videos</li>
          <li>Development: I’d like to upgrade the version of JavaFX Paintera uses, so it can run on my M1 Macbook. Some help needed setting up my dev env.</li>
        </ul>
      </li>
      <li>Zarpaint napari plugin. annotate directly into a zarr file.
        <ul>
          <li>We are currently extending zarpaint to add slice interpolation features, see https://github.com/GenevieveBuckley/slice-interpolation</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Big data handling
    <ul>
      <li>dask: <code class="language-plaintext highlighter-rouge">to_n5</code> function (to match <code class="language-plaintext highlighter-rouge">to_zarr</code>, <code class="language-plaintext highlighter-rouge">to_parquet</code>, etc.)</li>
      <li>dask-image: multi-resolution dataset generator (given a full resolution dataset, compute lower resolutions and save to N5/zarr file)</li>
      <li>dask-image: switch imread to use imageio instead of pims under the hood</li>
    </ul>
  </li>
</ul>

<h2 id="ryan-conrad-conradry">Ryan Conrad <a href="https://github.com/conradry">@conradry</a>
</h2>
<p>I’m a software engineer at the NIH focused on deep learning. Recent projects include the CEM500K and CEM1.5M datasets for unsupervised pre-training and transfer learning, the CEM-MitoLab dataset for training instance segmentation models to detect mitochondria, and the empanada library and corresponding napari plugin for panoptic segmentation of 2D and 3D EM data (especially FIBSEM). Lately, I’ve been transitioning my data pipeline to work with OME-Zarr and am very interested in the associated metadata discussion. Really, all of the projects are interesting to me but ML and annotation tools are near the top of the list. Another topic of interest, that wasn’t mentioned, are methods for integrating existing ML datasets with different label spaces/ontologies in order to train more robust and general models.</p>

<h2 id="draga-doncila-pop-dragadoncila">Draga Doncila Pop <a href="https://github.com/DragaDoncila">@DragaDoncila</a>
</h2>
<p>Computer science PhD student from Monash University in Melbourne, Australia, working on an interactive cell tracking annotation and segmentation framework, and napari core developer. I want to learn how current tools allow for interactive annotation on large volumes and how we can improve the standardization of data formats and their metadata for storage, transfer and visualization. 
Our ideal segmentation and tracking solution would be capable of taking natural annotations and corrections on volume sections and using those to predict unseen sections and correct existing ones. We would prefer to keep a relatively simple learning model for feature extraction in favor of a rapid and robust optimization model that can solve on the fly and incorporate human annotations for rapid re-solve.
In general:</p>
<ul>
  <li>Image data formats and metadata</li>
  <li>Training &amp; prediction on large volumes</li>
  <li>Annotation tools</li>
</ul>

<h2 id="jan-funke-funkey">Jan Funke <a href="https://github.com/funkey">@funkey</a>
</h2>
<p>Computer vision and machine learning researcher at Janelia, working on applications to all sorts of microscopy image analysis. In the context of this hackathon, we are interested in building interactive machine learning methods, which we call “<strong>life-supervised learning</strong>”.</p>

<p>A life-supervised machine learning method learns on the fly as a user annotates a volume. It provides incremental updates to so far not annotated areas to either proof-read or accept as-is. In such a system, there is no difference between manual annotation of structures of interest, ground-truth generation, and proof-reading: As an annotator opens a new dataset and starts segmenting structures of interest, a machine learning algorithm is trained in the background to continuously make predictions in so far unannotated areas. Just as for humans, the training of the model is a continuous, life-long process. Over time, the algorithm gradually improves and the annotator transitions from teaching the model to correcting errors made by the model due to model error or data noise. Importantly, and in contrast to conventional pipelines, the training signal for the model is highly focused throughout the whole lifetime: Only structures that were erroneously predicted need to be corrected, and only those corrections are used to refine the model.</p>

<p>Such a system could be realized by splitting the machine learning method into two parts: (1) a slow, high-capacity learner to predict semantically meaningful embeddings, and (2) a fast, low-capacity learner to turn the embeddings into the target (e.g., a semantic segmentation). This separation will allow the system to be:</p>

<ul>
  <li>
<strong>Interactive</strong>: Corrections made by a user should quickly proliferate to other, not annotated areas. This will be realized by the fast, low-capacity learner.</li>
  <li>
<strong>Consistently Improving</strong>: the system should keep learning as more annotations are available. This will be realized by the slow, high-capacity learner, which will keep continuously improving the semantic embeddings on all available annotations.</li>
  <li>
<strong>Persistent</strong>: User annotations should never be overwritten, even if other, similar structures have been annotated differently. This will be realized by extrapolation from annotated data points (i.e., transductive learning).</li>
</ul>

<h2 id="kyle-harrington-kephale">Kyle Harrington <a href="https://github.com/kephale">@kephale</a>
</h2>
<p>Computer scientist working as a senior research scientist at Oak Ridge National Lab. 
Web: https://kyleharrington.com</p>

<p>I have big zarr images on an HPC that need to be segmented, but no ground truth labels. At the hackathon I am cleaning up my workflow for general usage. In this workflow I use napari to interactively paint and train models + predict segmentations in HPC environments.</p>

<p>Topics of interest:</p>
<ul>
  <li>Image data formats and metadata</li>
  <li>Backend development</li>
  <li>Scalable ML [networks]</li>
  <li>ML finetuning and transfer learning</li>
  <li>Training and prediction on large volumes</li>
  <li>Annotation tools</li>
</ul>

<h2 id="larissa-heinrich-neptunes5thmoon">Larissa Heinrich <a href="https://github.com/neptunes5thmoon">@neptunes5thmoon</a>
</h2>
<p>I’m a PhD student in the Saalfeld lab working on machine learning problems for volume electron microscopy data. Specifically I have worked on super resolution to improve axial resolution in serial section data and segmentation of synapses and organelles.
I’m interested in developing network architectures that can effectively utilize large receptive fields. Further I’m hoping to work on transfer learning, self-supervised pre-training, finetuning or anything else that might help reduce the amount of training data we need. I also thoroughly enjoy discussions about the ongoing efforts regarding metadata standards.</p>

<h2 id="caleb-hulbert">Caleb Hulbert</h2>
<p>I am a software engineer in the Saalfeld lab, working primarily on Paintera, and helping maintain lab projects. My background was initially in biology, working in both a maize genetics lab, and a computational neuroscience lab in the past, before focusing more on software development. I’m interested in working on annotation tools for this hackathon, and engaging in active learning. I’m also interested in some of the ongoing efforts regarding image data formats and metadata.</p>

<h2 id="dagmar-kainmueller-kainmueller">Dagmar Kainmueller <a href="https://github.com/kainmueller">@kainmueller</a>
</h2>
<p>I am a computer scientist working on machine learning for image analysis, mainly instance- and panoptic segmentation. Towards generalizable / interactive ML, we’re looking into combining (or unifying?) components like cycleGAN, semi-supervised pipelines like momentum contrast, Bayesian DL, XAI, also RFs for fast user feedback, etc. Also very interested in existing / upcoming infrastructure (we’ve been playing with napari, paintera, ilastik) wondering where and how to best integrate our models</p>

<h2 id="dominik-kutra-k-dominiki">Dominik Kutra <a href="https://github.com/k-dominik">@k-dominiki</a>
</h2>
<p>I’m a research software engineer in Anna Kreshuk’s lab at EMBL Heidelberg. I work on <a href="www.ilastik.org">ilastik</a>, so accessible machine learning is close to my heart. I’m interested in making software easy to use (both for developers and non-coding users) - all aspects from algorithmic performance, UX/API design, deployment. The latter is especially challenging with the neural networks. Other topics I’m interested in:</p>
<ul>
  <li>Interoperability</li>
  <li>Interactive deep learning</li>
</ul>

<h2 id="caroline-malin-mayor">Caroline Malin-Mayor</h2>
<p>I am a computer vision researcher currently getting my PhD at University of Toronto. Previously I was a Scientific Computing Associate working in the Funke lab on cell tracking from sparsely annotated data in large light-sheet datasets of drosophila, zebrafish, and mouse. I am most interested in making this work more accessible to other researchers and end-users, which is challenging due to the size of the datasets and complexity of the algorithm. More generally, this falls under training/prediction on large volumes and backend development, as well as fine tuning the pre-trained models for application to new datasets.</p>

<h2 id="tri-nguyen">Tri Nguyen</h2>
<p>Hi! I have been a postdoc in the <a href="https://www.lee.hms.harvard.edu/">Lee Lab</a> at Harvard Medical School for about 4 years now working on segmentation of a cerebellum. Since I came to Janelia in 2018 to work with Jan on Daisy and then the culmination of an analysis of the cerebellum, I have pretty much touched on every aspects of segmentation and am now excited to work on decreasing the cost of segmentation through overcoming imaging defects/misalignments, transfer learning through cyclegan, superresolution, cheap groundtruthing, proofreading, etc. In one particular topic, I am excited to learn and brainstorm with people on how to tackle the issue of continual learning as it is well known in ML world that today’s architectures are prone to forget when exclusively presented with novel training samples.</p>

<h2 id="wei-ouyang-oeway">Wei Ouyang <a href="https://github.com/oeway">@oeway</a>
</h2>
<p>I am currently a researcher at Science for Life Laboratory and KTH Royal Insitute of Technology, Stockholm, Sweden.I developed <a href="https://imjoy.io">ImJoy</a> for building interactive tools using web and python. I am coordinating a collaborative effort of BioImage <a href="https://bioimage.io">Model Zoo</a> which aims to build a platform for sharing AI models in bioimage analysis. I am responsible for building cloud computing infrastructure to enable test runs of models in the BioImage Model Zoo. I am working on the BioEngine and Hypha for allowing scaling AI models and application serving.</p>

<p>For the hackathon, I am interested in discussing with the CellMap team and others about:</p>
<ul>
  <li>AI model sharing standard and the BioImage Model Zoo</li>
  <li>Scalable AI model serving (inference and training) in the cloud</li>
  <li>Interactive model training, for collaboration between research groups, public AI challenges or citizen science</li>
  <li>ImJoy, web-based image analysis, web assembly, web based image visualization and annotation</li>
</ul>

<h2 id="will-patton-pattonw">Will Patton <a href="https://github.com/pattonw">@pattonw</a>
</h2>
<p>I am a software engineer / machine learning practitioner interested in tooling and algorithm development for applying machine learning quickly, easily, and efficiently. I did my undergrad in Mathematics and Computational Modeling, and have worked at Janelia since graduating in May 2018. Since joining Janelia I have worked with a couple labs and project groups, most recently as a part of Scientific Computing and the CellMap project team. Of particular interest to me are the model related categories (finetuning/transfer learning, training/prediction on large volumes, scalable ML networks). I am very excited to collaborate with the other participants, and learn from their work.</p>

<h2 id="tobias-pietzsch-tpietzsch">Tobias Pietzsch <a href="https://github.com/tpietzsch">@tpietzsch</a>
</h2>
<p>I’m a computer scientist working as a freelancer for Scientific Computing at Janelia (among others). I develop and maintain ImgLib2 and BigDataViewer, and contribute to Mastodon, MaMuT, Labkit, etc. I’m interested in all aspects of image processing, analysis, visualization, and storage. (But in practice, I mostly work on tools for interactive visualization and annotation.) For the hackathon, I would like to see what everybody is working on. I would like to learn more about the active learning project, i.e., the big picture and where our tools fit in.
I would also love to do some hacking. Random topic selection:</p>
<ul>
  <li>support images with masks (alpha/binary) in ImgLib2/BigDataViewer,</li>
  <li>implement HTTP backend for N5</li>
  <li>support HDF5 backend for BDV on Apple ARM64</li>
  <li>finish work on improving BDV UI on HiDPI screens
I’m also willing to dive into OME-NGFF and metadata discussions, if I’m forced to… <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">
</li>
</ul>

<h2 id="stephan-preibisch-stephanpreibisch">Stephan Preibisch <a href="https://github.com/StephanPreibisch">@StephanPreibisch</a>
</h2>
<p>I am a computer scientist that also worked for some years experimentally with single molecule techniques. The major projects I contributed to so far are ImgLib2, BigStitcher, Render, RS-FISH, STIM and some more. I am the director of Scientific Computing at HHMI Janelia and very interested in reconstruction and analysis of very large (peta-scale) lightsheet and electron microscopy datasets, machine learning, and Open Science Software in general. For the hackathon I would like to get an overview of everyone’s work and would like to contribute to our new active learning project and integrate existing tools such as BigStitcher better with downstream tools for image segmentation.</p>

<h2 id="franz-rieger-riegerfr">Franz Rieger <a href="https://github.com/riegerfr">@riegerfr</a>
</h2>
<p>I’m a first year PhD student in Jörgen Kornfeld’s lab working on ML for connectomics. We try to take advantage of the vast amounts of unlabelled data via self supervised pre-training of segmentation models. To give back to the ML community we are aiming to improve the segmentation architectures and we are also working on more biologically plausible self-supervised learning approaches. During the hackathon I would like to explore novel segmentation approaches.</p>

<h2 id="konrad-rokicki-krokicki">Konrad Rokicki <a href="https://github.com/krokicki">@krokicki</a>
</h2>
<p>I’m the manager of software engineering in Scientific Computing, and I created the Janelia Workstation for FlyLight and MouseLight, for enabling end-user access to large collections of bioimage data sets. Davis, Aubrey and I have discussed prototyping support for CellMap data sets in Workstation. I’ve begun by adding functionality to make remote n5/zarr containers available for browsing in the Workstation, and Davis is working on creating a database for the data set metadata. The next step is for me to integrate BigDataViewer into the Workstation, to be able to quickly view any data set with the click of a button, without the user having to think about file paths or data locality.  During the hackathon, I’d like to benefit from the groups’ collective experience and insight to guide my integration of BDV, and to discuss what other high-level functionality might be useful for further enabling accessibility.</p>

<h2 id="stephan-saalfeld-axtimwalde">Stephan Saalfeld <a href="https://github.com/axtimwalde">@axtimwalde</a>
</h2>
<p>Computer Scientist interested in image analysis (of neurons and other cells, also any other image data) and open source software.  Really likes the size, dazzling richness, and confusing artifacts of large 3D electron microscopy.  Wrote some parts of ImgLib2, BigDataViewer, Paintera, and a few other open source tools.  Currently group leader at Janelia working on scalable image analysis of large 3D EM data together with CellMap project team and others.  I would love to learn about everybody else’s work and integrate your ideas and tools with our ideas and tools, e.g. have Paintera interactively generate cellular organelle predictions, edit them and start retraining, or interactively use contour predictors to aid manual annotation.</p>

<h2 id="deborah-schmidt-frauzufall">Deborah Schmidt <a href="https://github.com/frauzufall">@frauzufall</a>
</h2>
<p>Research Software Engineer for Helmholtz Imaging with experience in human computer interaction, data visualization, ImageJ2 / SciJava core development, usability, DevOps, reproducibility. Together with Martin I was part of a team publishing a <a href="https://rupress.org/jcb/article/220/2/e202010039/211599/3D-FIB-SEM-reconstruction-of-microtubule-organelle">whole beta cell reconstruction</a>, including Blender visualizations. I work with Kyle on <a href="https://rupress.org/jcb/article/220/2/e202010039/211599/3D-FIB-SEM-reconstruction-of-microtubule-organelle">Album</a> for sharing reproducible software solutions via single python files.</p>

<p>Looking forward to meeting people and potentially working on the Label Editor (label visualization based on BDV), integrating the existing beta cell segmentation strategies into Album solutions, compatibility of Album solutions with other frameworks, alo happy to do visualizations and open to other ideas.</p>

<h2 id="arlo-sheridan-sheridana">Arlo Sheridan <a href="https://github.com/sheridana">@sheridana</a>
</h2>
<p>Machine learning researcher working on various segmentation, restoration and multi-object tracking projects at the Salk Institute. Previously worked in the Funke lab at Janelia on neuron segmentation in large EM volumes. I am interested in generalizable/scalable networks, transfer + active learning, and visualization.</p>

<h2 id="aubrey-weigel-avweigel">Aubrey Weigel <a href="https://github.com/avweigel">@avweigel</a>
</h2>
<p>I started grad school as a physicist building microscopes to study diffusion of protein channels on the plasma membrane. I then spent my postdoc looking inside the cell at the ER, Golgi, and everything in between. Over time I’ve shifted from using light microscopy to study dynamics in living systems to using electron microscopy to study morphologies and ultrastructure in ‘not-so-living’ systems. I now lead the <a href="https://www.janelia.org/project-team/cellmap">CellMap project team</a>, an evolution of the <a href="https://www.janelia.org/project-team/cosem">COSEM project team</a>, to interrogate the 3D architecture and subcellular structures of cells in tissues using approaches like volume EM, CLEM, and expansion microscopy. Maybe not surprisingly, I’m interested - as both a user and project manager - in all areas of the proposed topics.</p>

<h2 id="martin-weigert-maweigert">Martin Weigert <a href="https://github.com/maweigert">@maweigert</a>
</h2>
<p>I am a computer scientist with an interest in machine learning based image reconstruction and segmentation, as well as the development of open source python tools such as CSBDeep and stardist. I am currently a group leader at EPFL, where my team is focused on developing unsupervised methods for ml-based bioimage analysis and the extension of segmentation methods to different data modalities. For the hackathon, I am (above all) interested in exchange of scientific ideas and discussing fundamental limitations of current approaches (both software and methods wise).</p>

<h2 id="marwan-zouinkhi-mzouink">Marwan Zouinkhi <a href="https://github.com/mzouink">@mzouink</a>
</h2>
<p>I started as a software engineer and a mobile developer then I shifted into scaling image analysis for my PHD. Last November, I joined Janelia as part of the Scientific computing team and CellMap project. I am interested in scaling image processing of large multi-dimensional datasets and automatizing image reconstructions. Currently, I am working on a versioned data store service that will be used in the active learning project. During the hackathon I plan to integrate the versioned storage into Paintera and ML applications.</p>

<p><img src="/cellmap-hackathon-blog/images/group-dinner-1" alt="" title="group dinner">
<img src="/cellmap-hackathon-blog/images/group-dinner-2" alt="" title="group dinner"></p>

  </div>
<a class="u-url" href="/cellmap-hackathon-blog/introductions/about/2022/06/06/introductions.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/cellmap-hackathon-blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/cellmap-hackathon-blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/cellmap-hackathon-blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>News and chatter about the CellMap hackathon.</p>
      </div>
    </div>

    <div class="social-links">
<ul class="social-media-list">
<li><a rel="me" href="https://github.com/janelia-cellmap" target="_blank" title="janelia-cellmap"><svg class="svg-icon grey"><use xlink:href="/cellmap-hackathon-blog/assets/minima-social-icons.svg#github"></use></svg></a></li>
<li><a rel="me" href="https://twitter.com/search?q=%23CellMap&amp;src=typed_query&amp;f=top" target="_blank" title="search?q=%23CellMap&amp;src=typed_query&amp;f=top"><svg class="svg-icon grey"><use xlink:href="/cellmap-hackathon-blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li>
</ul>
</div>

  </div>

</footer>
</body>

</html>
